import pandas as pd
import m2cgen as m2c
import numpy as np
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.model_selection import train_test_split, GridSearchCV
from format_data import generate_features_from_file

# ==========================================
# 1. PREPARE DATA
# ==========================================

files = [
    ("Data/Leaning.txt", 3),
    ("Data/Leaning2.txt", 3),
    ("Data/Profi.txt", 0),
    ("Data/Profi2.txt", 0),
    ("Data/Profi3.txt", 0),
    ("Data/zuleicht.txt", 1),
    ("Data/zuleicht2.txt", 1)
]

data = []

# Assuming generate_features_from_file returns a list of lists or similar
for f in files:
    data.extend(generate_features_from_file(f[0], f[1]))

print(len(data))

df = pd.DataFrame(data, columns=['std_dev_z', 'recoil_proxy_z', 'recoil_proxy_x', 'recoil_proxy_y', "recoil_proxy", 'rescuer_hr', 'tilt_error', "wobble_score_x", "wobble_score_y", "wobble_score_z", "label"])
X = df[['std_dev_z', 'recoil_proxy_z', 'recoil_proxy_x', 'recoil_proxy_y', "recoil_proxy", 'rescuer_hr', 'tilt_error', "wobble_score_x", "wobble_score_y", "wobble_score_z"]]
y = df['label']

# Split Hold-out Test Set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==========================================
# 2. GENERATE ALPHAS
# ==========================================

clf = DecisionTreeClassifier(random_state=42)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas[:-1] # Remove the single root node option

print(f"Found {len(ccp_alphas)} potential alphas for pruning.")

# ==========================================
# 3. K-FOLD CROSS VALIDATION
# ==========================================

grid_search = GridSearchCV(
    estimator=DecisionTreeClassifier(random_state=42),
    param_grid={'ccp_alpha': ccp_alphas},
    cv=5,
    scoring='accuracy'
)

print("Running 5-Fold Cross Validation on all alphas...")
grid_search.fit(X_train, y_train)

# ==========================================
# 4. GET THE WINNER
# ==========================================

best_clf = grid_search.best_estimator_
best_alpha = grid_search.best_params_['ccp_alpha']

print(f"\nWINNER FOUND:")
print(f"Best Alpha: {best_alpha:.5f}")
print(f"CV Score:   {grid_search.best_score_:.2f}")
print(f"Test Score: {best_clf.score(X_test, y_test):.2f}")
print(f"Leaves:     {best_clf.get_n_leaves()}")

# ==========================================
# 5. EXPORT LOGIC (FIXED)
# ==========================================

print("\n--- RULES ---")
print(export_text(best_clf, feature_names=['std_dev_z', 'recoil_proxy_z', 'recoil_proxy_x', 'recoil_proxy_y', "recoil_proxy", 'rescuer_hr', 'tilt_error', "wobble_score_x", "wobble_score_y", "wobble_score_z"]))

print("\n--- ARKTS CODE ---")

# Export the raw scoring function (returns array of probabilities)
js_code = m2c.export_to_javascript(best_clf)

# We wrap the generated code in a function that finds the max probability
print("private static runDecisionTree(input: number[]): number[] {") # <--- Changed return type to number[]
print("    // 1. Define the scoring function generated by m2cgen")
print(js_code) 
print("    // 2. Get the probabilities")
print("    let probs = score(input);")
print("")
print("    // 3. Return the full probability array")
print("    return probs;") # <--- Changed to return the array directly
print("}")